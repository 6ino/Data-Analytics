{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: KNN on IRIS dataset and on PCA of iris dataset\n",
    "\n",
    "In this assignment, we will first learn how to use KNN algorithms. \n",
    "\n",
    "(1) We will Analyze the iris dataset with the original features i.e. sepal length, sepal width, petal length and petal width with KNN as the first task. Try with neighbor values as 1, 3, 5, 7 etc and see the difference in accuracy? out of the four knn values which one gives the highest accuracy?\n",
    "\n",
    "(2)The second task will be to analyze the the iris dataset but based on the PCA components as features instead of original features. To perform the second task, first you would need to perform PCA on the iris dataset (on the features, NOT on the class labels) with two PCA components. You can use euclidean distance as the distance metric. You can use the KNeighborsClassifier() function from sklearn in these two tasks.\n",
    "\n",
    "(3)The third task would be to implement your own knn algorithm, euclidean distance metric and plug them in the algorithm for task1 and task2 and repeat those tasks.\n",
    "Verify that the results obtained using the KNeighborsClassifier() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN : sklearn\n",
    "\n",
    "Go to https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html. The page contains nice description of KNN. The try to run the following code and modify the ipython notebook to answer few questions. You can insert markdown cells for answers, or code cells for programming. Run the cells one by one. Inspect the code. Write code and answers following the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.3.1.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris data Feature Names :  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n"
     ]
    }
   ],
   "source": [
    "#Load iris data\n",
    "iris = load_iris()\n",
    "# check what are the features\n",
    "print(\"Iris data Feature Names : \", iris.feature_names)\n",
    "\n",
    "# print the detailed description \n",
    "print(iris.DESCR)\n",
    "\n",
    "# load the data and target\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "1.Create a train test split with test size as 20% of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#define the number of neighbors to use for the KNN\n",
    "k=3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# In KNN method we do not have any actual model.\n",
    "# So fitting the training data to KNN means that the KNN algorithm will remember the entire dataset\n",
    "# as the training data\n",
    "\n",
    "# write code here to fit the training to the KNN with number of neighbors or k value as 3\n",
    "classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# modify code to predict the label of the test data\n",
    "predicted = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** a) Write a code to to print accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1 is: 0.9800\n",
      "Accuracy for k=3 is: 0.9800\n",
      "Accuracy for k=5 is: 0.9800\n",
      "Accuracy for k=7 is: 0.9800\n",
      "\n",
      "Maximum accuracy 0.9800\n"
     ]
    }
   ],
   "source": [
    "# Answer for question 1 : \n",
    "# write your code here\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k_values = [1, 3, 5, 7]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Accuracy for k={k} is: {acc:.4f}\")\n",
    "\n",
    "max_accuracy = max(accuracies)\n",
    "best_k = k_values[accuracies.index(max_accuracy)]\n",
    "\n",
    "print(f\"\\nMaximum accuracy {max_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of the KNN method for different values of k\n",
    "\n",
    "**Question 1** b). Perform KNN with different k values e.g. 1, 3, 5 and 7. Report which one gives the highest accuracy on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (best_k) #They all give 98% accuracy for the 4 K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of the KNN method for different values of k\n",
    "\n",
    "**Question 2**. The second task will be to analyze the the iris dataset but based on the PCA components as features instead of original features. To perform the second task, first you would need to perform PCA on the iris dataset with principal component value as 2. [take help from the pca assignment that we did on Tuesday ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of the KNN method for different values of k\n",
    "\n",
    "**Question 2**. Use the two PCA components that you obtained from the iris dataset as the two new features of iris dataset. create train-test split on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with PCA) for k=1 is: 0.9600\n",
      "Accuracy (with PCA) for k=3 is: 1.0000\n",
      "Accuracy (with PCA) for k=5 is: 1.0000\n",
      "Accuracy (with PCA) for k=7 is: 1.0000\n",
      "\n",
      "The best k-value for maximum accuracy (with PCA) 1.0000 is: 3\n"
     ]
    }
   ],
   "source": [
    "accuracies_pca = []\n",
    "\n",
    "for k in k_values:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(X_pca_train, y_train)\n",
    "    predictions = classifier.predict(X_pca_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    accuracies_pca.append(acc)\n",
    "    print(f\"Accuracy (with PCA) for k={k} is: {acc:.4f}\")\n",
    "\n",
    "max_accuracy_pca = max(accuracies_pca)\n",
    "best_k_pca = k_values[accuracies_pca.index(max_accuracy_pca)]\n",
    "print(f\"\\nThe best k-value for maximum accuracy (with PCA) {max_accuracy_pca:.4f} is: {best_k_pca}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KNN accuracy for k=3 is: 0.9800\n",
      "Custom KNN accuracy (with PCA) for k=3 is: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, test_point, k):\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(X_train[i], test_point)\n",
    "        distances.append((i, dist))\n",
    "    sorted_distances = sorted(distances, key=lambda x: x[1])\n",
    "    k_nearest = [y_train[d[0]] for d in sorted_distances[:k]]\n",
    "    return max(set(k_nearest), key=k_nearest.count)\n",
    "\n",
    "# Test our KNN with the original dataset:\n",
    "k=3\n",
    "custom_predictions = [knn_predict(X_train, y_train, x, k) for x in X_test]\n",
    "custom_accuracy = accuracy_score(y_test, custom_predictions)\n",
    "print(f\"Custom KNN accuracy for k={k} is: {custom_accuracy:.4f}\")\n",
    "\n",
    "# Test our KNN with the PCA dataset:\n",
    "custom_predictions_pca = [knn_predict(X_pca_train, y_train, x, k) for x in X_pca_test]\n",
    "custom_accuracy_pca = accuracy_score(y_test, custom_predictions_pca)\n",
    "print(f\"Custom KNN accuracy (with PCA) for k={k} is: {custom_accuracy_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN algorithm is powerful when making decisions that when two rows of data have similar attributes we can take the neighbors of the data point and predict outcomes for the datapoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While library functions make implementations smoother and faster, building things from scratch gives a more in-depth knowledge of the subject.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
